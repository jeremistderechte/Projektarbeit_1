{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aef91707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"New_York_Times_labeled_dataframe_manual_cleaned.csv\")\n",
    "data = data[data[\"ranges\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38c93ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>ranges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>in 2008, hillary clinton was defeated in the d...</td>\n",
       "      <td>hillary clinton, barack obama, obama's, hillar...</td>\n",
       "      <td>[[9, 24], [68, 80], [224, 231], [320, 327], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"the deeper reason putin seduces is that he be...</td>\n",
       "      <td>putin</td>\n",
       "      <td>[[19, 24], [138, 143], [549, 554], [1033, 1038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i'll give ann coulter points for being forthri...</td>\n",
       "      <td>ann coulter</td>\n",
       "      <td>[[10, 21]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>you excuse trump’s tweets as less than artful ...</td>\n",
       "      <td>trump’s, mr. comey</td>\n",
       "      <td>[[11, 18], [144, 151], [63, 72]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>quite frankly, i don't think it's too hard to ...</td>\n",
       "      <td>michael cohen</td>\n",
       "      <td>[[55, 68]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>491</td>\n",
       "      <td>to answer the question, america will become ev...</td>\n",
       "      <td>trump, ginsburg, trump's</td>\n",
       "      <td>[[461, 466], [356, 364], [277, 284]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>493</td>\n",
       "      <td>some of us don't pine for the dignity of a roy...</td>\n",
       "      <td>obamas, mr. obama, george w. bush</td>\n",
       "      <td>[[149, 155], [322, 331], [379, 393]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>for some reasons, mr. trump has come to believ...</td>\n",
       "      <td>mr. trump, mcmaster,</td>\n",
       "      <td>[[18, 27], [511, 520], [591, 600], [940, 949],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>to be an american citizen with the right to vo...</td>\n",
       "      <td>trump</td>\n",
       "      <td>[[514, 519]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>well, what kind of lawyer do you think trump w...</td>\n",
       "      <td>trump, mueller</td>\n",
       "      <td>[[39, 44], [153, 158], [362, 367], [267, 274]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           sentence  \\\n",
       "0             0  in 2008, hillary clinton was defeated in the d...   \n",
       "1             1  \"the deeper reason putin seduces is that he be...   \n",
       "4             4  i'll give ann coulter points for being forthri...   \n",
       "7             7  you excuse trump’s tweets as less than artful ...   \n",
       "8             8  quite frankly, i don't think it's too hard to ...   \n",
       "..          ...                                                ...   \n",
       "491         491  to answer the question, america will become ev...   \n",
       "493         493  some of us don't pine for the dignity of a roy...   \n",
       "495         495  for some reasons, mr. trump has come to believ...   \n",
       "496         496  to be an american citizen with the right to vo...   \n",
       "497         497  well, what kind of lawyer do you think trump w...   \n",
       "\n",
       "                                                labels  \\\n",
       "0    hillary clinton, barack obama, obama's, hillar...   \n",
       "1                                                putin   \n",
       "4                                          ann coulter   \n",
       "7                                   trump’s, mr. comey   \n",
       "8                                        michael cohen   \n",
       "..                                                 ...   \n",
       "491                           trump, ginsburg, trump's   \n",
       "493                  obamas, mr. obama, george w. bush   \n",
       "495                              mr. trump, mcmaster,    \n",
       "496                                              trump   \n",
       "497                                     trump, mueller   \n",
       "\n",
       "                                                ranges  \n",
       "0    [[9, 24], [68, 80], [224, 231], [320, 327], [4...  \n",
       "1    [[19, 24], [138, 143], [549, 554], [1033, 1038...  \n",
       "4                                           [[10, 21]]  \n",
       "7                     [[11, 18], [144, 151], [63, 72]]  \n",
       "8                                           [[55, 68]]  \n",
       "..                                                 ...  \n",
       "491               [[461, 466], [356, 364], [277, 284]]  \n",
       "493               [[149, 155], [322, 331], [379, 393]]  \n",
       "495  [[18, 27], [511, 520], [591, 600], [940, 949],...  \n",
       "496                                       [[514, 519]]  \n",
       "497     [[39, 44], [153, 158], [362, 367], [267, 274]]  \n",
       "\n",
       "[235 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "531a0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "import json\n",
    "\n",
    "training_test_text = []\n",
    "\n",
    "training_test_entity = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    range_word = json.loads(row[\"ranges\"])\n",
    "    temp_list = []\n",
    "    \n",
    "    if len(range_word) == 1:\n",
    "        range_word = range_word[0]\n",
    "        range_word.append(\"PER_CUSTOM\")\n",
    "        entity_dict ={\"entities\" : [tuple(range_word)]}\n",
    "    else:\n",
    "        for i, inner_range in enumerate(range_word):\n",
    "            inner_range.append(\"PER_CUSTOM\")\n",
    "            temp_list.append(tuple(inner_range))\n",
    "        range_word = temp_list\n",
    "        entity_dict = {\"entities\" : tuple(range_word)}\n",
    "    \n",
    "    training_test_text.append(text)\n",
    "\n",
    "    training_test_entity.append(entity_dict)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b84645e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_entity, test_entity = train_test_split(training_test_text, \n",
    "                                                                   training_test_entity, \n",
    "                                                                   test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d41f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ((9, 24, 'PER_CUSTOM'),\n",
       "  (68, 80, 'PER_CUSTOM'),\n",
       "  (224, 231, 'PER_CUSTOM'),\n",
       "  (320, 327, 'PER_CUSTOM'),\n",
       "  (429, 436, 'PER_CUSTOM'),\n",
       "  (515, 522, 'PER_CUSTOM'),\n",
       "  (800, 807, 'PER_CUSTOM'),\n",
       "  (447, 452, 'PER_CUSTOM'),\n",
       "  (625, 630, 'PER_CUSTOM'),\n",
       "  (764, 769, 'PER_CUSTOM'))}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_entity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "001ec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_for = \"{'entities': ((16, 19, 'PER_CUSTOM'), (117, 120, 'PER_CUSTOM'))}\"\n",
    "for i, labels in enumerate(train_entity):\n",
    "    if str(labels) == search_for:\n",
    "        print(\"Found at\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a39c3b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i do believe it might. who would have thought that paula jones and then ms. lewinsky would be 'slick willy's' undoing. i believe the only reason clinton was not found guilty during his impeachment had to do with sexual indiscretions by other democrat senators, namely for starters ted kennedy and mr. lynda byrd johnson [chuck robb].\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93c61f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "def create_pipeline():\n",
    "    model = None\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loads model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        print(\"get pipeline\")\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    return ner, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af5483c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i do believe it might. who would have thought that...\" with entities \"((51, 62, 'PER_CUSTOM'), (72, 84, 'PER_CUSTOM'), (...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"brooks is so incorrect in his analysis of trump an...\" with entities \"((0, 6, 'PER_CUSTOM'), (42, 47, 'PER_CUSTOM'), (96...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"donald trump is not \"tough enough\" on any issue. h...\" with entities \"((0, 12, 'PER_CUSTOM'), (178, 192, 'PER_CUSTOM'), ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"far from continuing a trump trend when trump's gon...\" with entities \"((22, 27, 'PER_CUSTOM'), (134, 139, 'PER_CUSTOM'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Losses {'ner': 1305.7351313332958}\n",
      "Iteration 2 Losses {'ner': 200.76379404405208}\n",
      "Iteration 3 Losses {'ner': 81.84224185969514}\n",
      "Iteration 4 Losses {'ner': 46.41342233154541}\n",
      "Iteration 5 Losses {'ner': 42.99864474590217}\n",
      "Iteration 6 Losses {'ner': 30.478588733891943}\n"
     ]
    }
   ],
   "source": [
    "ner, nlp = create_pipeline()\n",
    "n_iter = 6\n",
    "loss_graph_list = []\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "    losses = {}\n",
    "    for text, annotations in zip(train_text, train_entity):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "        \n",
    "    print(\"Iteration\", itn + 1, \"Losses\", losses)\n",
    "    loss_graph_list.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bd8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa11a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to save comment\n",
    "#import pickle\n",
    "#with open(\"./models/custom_model_NYT.pickel\", \"wb\") as f:\n",
    "#    pickle.dump(nlp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8d13f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=[\"sentence\", \"label\"])\n",
    "\n",
    "df_test[\"sentence\"] = test_text\n",
    "df_test[\"label\"] = test_entity\n",
    "\n",
    "df_test.to_csv(\"test_data_NYT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a9f64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Default scoring pipeline\n",
    "scorer = Scorer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb948c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERmodel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.nlp = None\n",
    "        \n",
    "    def __create_pipeline():\n",
    "        model = None\n",
    "        if model is not None:\n",
    "            nlp = spacy.load(model)\n",
    "            print(\"Loads model '%s'\" % model)\n",
    "        else:\n",
    "            nlp = spacy.blank(\"en\")\n",
    "            print(\"Created blank 'en' model\")\n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe(\"ner\")\n",
    "        else:\n",
    "            print(\"get pipeline\")\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        return ner, nlp\n",
    "        \n",
    "    def train(epochs):\n",
    "        ner, nlp = __create_pipeline()\n",
    "        n_iter = epochs\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        with nlp.disable_pipes(other_pipes):\n",
    "            optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            losses = {}\n",
    "            for text, annotations in zip(train_text, train_entity):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], losses=losses)\n",
    "\n",
    "            print(\"Iteration\", itn + 1, \"Losses\", losses)\n",
    "            \n",
    "    def save_model(modelpath):\n",
    "        with open(modelpath, \"wb\") as f:\n",
    "            pickle.dump(nlp, f)\n",
    "    \n",
    "    def load_model(modelpath):\n",
    "        with open(modelpath, \"rb\") as f:\n",
    "            nlp = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
