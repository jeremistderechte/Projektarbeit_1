{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef91707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"twitter_labeled_dataframe.csv\")\n",
    "data = data[data[\"ranges\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c93ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>ranges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>you can apply for compensation here if you had...</td>\n",
       "      <td>josh</td>\n",
       "      <td>[[99, 103]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>we'd be happy to look into this issue for you....</td>\n",
       "      <td>lindsey</td>\n",
       "      <td>[[100, 107]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>hi andrew. this is great to hear. which train ...</td>\n",
       "      <td>andrew, josh</td>\n",
       "      <td>[[3, 9], [116, 120]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>nora culik</td>\n",
       "      <td>nora culik</td>\n",
       "      <td>[[0, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>we request you to share your alternate contact...</td>\n",
       "      <td>sneha</td>\n",
       "      <td>[[117, 122]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>928</td>\n",
       "      <td>hey michael, we appreciate your feedback! plea...</td>\n",
       "      <td>michael</td>\n",
       "      <td>[[4, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>931</td>\n",
       "      <td>our sincerest apologies, dave. be sure to reac...</td>\n",
       "      <td>dave</td>\n",
       "      <td>[[25, 29]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>939</td>\n",
       "      <td>hi chunhua, we have just responded to your dm ...</td>\n",
       "      <td>chunhua</td>\n",
       "      <td>[[3, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>942</td>\n",
       "      <td>oh no, jeff. sorry to hear about this terrible...</td>\n",
       "      <td>jeff</td>\n",
       "      <td>[[7, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>hi. please can you dm me your query as i'm str...</td>\n",
       "      <td>chloe</td>\n",
       "      <td>[[80, 85]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           sentence  \\\n",
       "9             9  you can apply for compensation here if you had...   \n",
       "10           10  we'd be happy to look into this issue for you....   \n",
       "13           13  hi andrew. this is great to hear. which train ...   \n",
       "16           16                                         nora culik   \n",
       "18           18  we request you to share your alternate contact...   \n",
       "..          ...                                                ...   \n",
       "928         928  hey michael, we appreciate your feedback! plea...   \n",
       "931         931  our sincerest apologies, dave. be sure to reac...   \n",
       "939         939  hi chunhua, we have just responded to your dm ...   \n",
       "942         942  oh no, jeff. sorry to hear about this terrible...   \n",
       "990         990  hi. please can you dm me your query as i'm str...   \n",
       "\n",
       "           labels                ranges  \n",
       "9            josh           [[99, 103]]  \n",
       "10        lindsey          [[100, 107]]  \n",
       "13   andrew, josh  [[3, 9], [116, 120]]  \n",
       "16     nora culik             [[0, 10]]  \n",
       "18          sneha          [[117, 122]]  \n",
       "..            ...                   ...  \n",
       "928       michael             [[4, 11]]  \n",
       "931          dave            [[25, 29]]  \n",
       "939       chunhua             [[3, 10]]  \n",
       "942          jeff             [[7, 11]]  \n",
       "990         chloe            [[80, 85]]  \n",
       "\n",
       "[129 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531a0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "import json\n",
    "\n",
    "training_test_text = []\n",
    "\n",
    "training_test_entity = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    range_word = json.loads(row[\"ranges\"])\n",
    "    temp_list = []\n",
    "    \n",
    "    if len(range_word) == 1:\n",
    "        range_word = range_word[0]\n",
    "        range_word.append(\"PER_CUSTOM\")\n",
    "        entity_dict ={\"entities\" : [tuple(range_word)]}\n",
    "    else:\n",
    "        for i, inner_range in enumerate(range_word):\n",
    "            inner_range.append(\"PER_CUSTOM\")\n",
    "            temp_list.append(tuple(inner_range))\n",
    "        range_word = temp_list\n",
    "        entity_dict = {\"entities\" : tuple(range_word)}\n",
    "    \n",
    "    training_test_text.append(text)\n",
    "\n",
    "    training_test_entity.append(entity_dict)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84645e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_entity, test_entity = train_test_split(training_test_text, \n",
    "                                                                   training_test_entity, \n",
    "                                                                   test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001ec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_for = \"{'entities': ((16, 19, 'PER_CUSTOM'), (117, 120, 'PER_CUSTOM'))}\"\n",
    "for i, labels in enumerate(train_entity):\n",
    "    if str(labels) == search_for:\n",
    "        print(\"Found at\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39c3b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you're welcome, ken! we realize that buffering can be a frustrating thing to endure, so no worries. have a great weekend! ðŸ’š\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c61f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)\n",
    "from spacy.training import Example\n",
    "\n",
    "def create_pipeline():\n",
    "    model = None\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loads model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        print(\"get pipeline\")\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    return ner, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af5483c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"you're welcome, ken! we realize that buffering can...\" with entities \"((16, 19, 'PER_CUSTOM'), (116, 119, 'PER_CUSTOM'))\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Losses {'ner': 203.36983752432224}\n",
      "Iteration 2 Losses {'ner': 22.326144694480472}\n",
      "Iteration 3 Losses {'ner': 9.826959182029421}\n"
     ]
    }
   ],
   "source": [
    "ner, nlp = create_pipeline()\n",
    "n_iter = 3\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "    losses = {}\n",
    "    for text, annotations in zip(train_text, train_entity):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "        \n",
    "    print(\"Iteration\", itn + 1, \"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa11a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./models/custom_model_twitter.pickel\", \"wb\") as f:\n",
    "    pickle.dump(nlp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d13f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=[\"sentence\", \"label\"])\n",
    "\n",
    "df_test[\"sentence\"] = test_text\n",
    "df_test[\"label\"] = test_entity\n",
    "\n",
    "df_test.to_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a9f64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Default scoring pipeline\n",
    "scorer = Scorer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb948c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERmodel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.nlp = None\n",
    "        \n",
    "    def __create_pipeline():\n",
    "        model = None\n",
    "        if model is not None:\n",
    "            nlp = spacy.load(model)\n",
    "            print(\"Loads model '%s'\" % model)\n",
    "        else:\n",
    "            nlp = spacy.blank(\"en\")\n",
    "            print(\"Created blank 'en' model\")\n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe(\"ner\")\n",
    "        else:\n",
    "            print(\"get pipeline\")\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        return ner, nlp\n",
    "        \n",
    "    def train(epochs):\n",
    "        ner, nlp = __create_pipeline()\n",
    "        n_iter = epochs\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        with nlp.disable_pipes(other_pipes):\n",
    "            optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            losses = {}\n",
    "            for text, annotations in zip(train_text, train_entity):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], losses=losses)\n",
    "\n",
    "            print(\"Iteration\", itn + 1, \"Losses\", losses)\n",
    "            \n",
    "    def save_model(modelpath):\n",
    "        with open(modelpath, \"wb\") as f:\n",
    "            pickle.dump(nlp, f)\n",
    "    \n",
    "    def load_model(modelpath):\n",
    "        with open(modelpath, \"rb\") as f:\n",
    "            nlp = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
